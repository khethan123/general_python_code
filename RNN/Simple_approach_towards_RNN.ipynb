{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr/TrSkHAGmy2clZ0phxph"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple RNN**\n",
        "An implementation of a minimal character-level Vanilla Recurrent Neural Network (RNN) model for text generation. It's written in Python and uses NumPy for numerical operations."
      ],
      "metadata": {
        "id": "c56GYQ5q8j2p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7p_rWyCqs9up"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
        "BSD License\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading & Preprocessing**\n"
      ],
      "metadata": {
        "id": "DDjYXIzB84XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read and preprocess the Shakespearean sonnets\n",
        "# remember to keep a simple plain text file\n",
        "with open('/content/sonnet.txt', 'r', encoding='utf-8') as file:\n",
        "    sonnets_text = file.read()\n",
        "\n",
        "# Split the text into training and validation sets\n",
        "total_length = len(sonnets_text)\n",
        "split_ratio = 0.8  # 80% for training, 20% for validation\n",
        "\n",
        "split_index = int(total_length * split_ratio)\n",
        "training_data = sonnets_text[:split_index]\n",
        "validation_data = sonnets_text[split_index:]\n",
        "\n",
        "# Save the training and validation datasets to files\n",
        "with open('training.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(training_data)\n",
        "\n",
        "with open('validation.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(validation_data)\n"
      ],
      "metadata": {
        "id": "mkiUuH6azo_x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data I/O and Vocabulary Setup:**\n",
        "\n",
        "The code reads the training text, creates a set of unique characters in the text, and assigns indices to each character. It also calculates the sizes of the data and vocabulary. This vocabulary setup is crucial for encoding characters into numerical inputs for the RNN.\n",
        "\n",
        "### **Hyperparameters and Model Parameters:**\n",
        "\n",
        "Hyperparameters like `hidden_size` (size of the hidden layer), `seq_length` (number of steps to unroll the RNN for), and `learning_rate` are defined. Model parameters like weights and biases `(Wxh, Whh, Why, bh, by)` are initialized using random values."
      ],
      "metadata": {
        "id": "ucBY7Lnr9W44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data I/O\n",
        "data = open('/content/training.txt', 'r').read()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size =  256 # Size of hidden layer of neurons\n",
        "seq_length = 25 # Number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# Model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # Input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # Hidden to output\n",
        "bh = np.zeros((hidden_size, 1))  # Hidden bias\n",
        "by = np.zeros((vocab_size, 1))  # Output bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuBVkBcLtL39",
        "outputId": "92c1e2c4-49a7-4d53-93aa-d8b11557ed43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 4825 characters, 30 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss Function (lossFun):**\n",
        "\n",
        "This function calculates the loss, gradients, and the last hidden state for a given input sequence and target sequence. It performs both the forward pass (calculating intermediate values) and the backward pass (backpropagation to compute gradients)."
      ],
      "metadata": {
        "id": "i3L2fLGj93Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lossFun(inputs, targets, hprev):\n",
        "    \"\"\"\n",
        "    Calculate loss, gradients, and last hidden state.\n",
        "\n",
        "    Args:\n",
        "        inputs (list of int): List of input character indices.\n",
        "        targets (list of int): List of target character indices.\n",
        "        hprev (numpy.ndarray): Initial hidden state.\n",
        "\n",
        "    Returns:\n",
        "        float: Loss\n",
        "        numpy.ndarray: Gradient for input to hidden weights\n",
        "        numpy.ndarray: Gradient for hidden to hidden weights\n",
        "        numpy.ndarray: Gradient for hidden to output weights\n",
        "        numpy.ndarray: Gradient for hidden bias\n",
        "        numpy.ndarray: Gradient for output bias\n",
        "        numpy.ndarray: Last hidden state\n",
        "    \"\"\"\n",
        "    xs, hs, ys, ps = {}, {}, {}, {}\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    loss = 0\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        xs[t] = np.zeros((vocab_size, 1))  # Encode in 1-of-k representation\n",
        "        xs[t][inputs[t]] = 1\n",
        "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)  # Hidden state\n",
        "        ys[t] = np.dot(Why, hs[t]) + by  # Unnormalized log probabilities for next chars\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # Probabilities for next chars\n",
        "        loss += -np.log(ps[t][targets[t], 0])  # Softmax (cross-entropy loss)\n",
        "\n",
        "    # Backward pass: Compute gradients going backwards\n",
        "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1  # Backprop into y\n",
        "        dWhy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Why.T, dy) + dhnext  # Backprop into h\n",
        "        dhraw = (1 - hs[t] * hs[t]) * dh  # Backprop through tanh nonlinearity\n",
        "        dbh += dhraw\n",
        "        dWxh += np.dot(dhraw, xs[t].T)\n",
        "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "        dhnext = np.dot(Whh.T, dhraw)\n",
        "\n",
        "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam)  # Clip to mitigate exploding gradients\n",
        "\n",
        "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]"
      ],
      "metadata": {
        "id": "hHOIM7MKtQYm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sampling Function (sample):**\n",
        "\n",
        "This function generates a sequence of characters by repeatedly sampling characters based on the RNN's predictions. It takes an initial memory state `h`, a seed character index `seed_ix`, and the number of characters to sample `n`."
      ],
      "metadata": {
        "id": "ZF8T15xm-AB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(h, seed_ix, n):\n",
        "    \"\"\"\n",
        "    Sample a sequence of integers from the model.\n",
        "\n",
        "    Args:\n",
        "        h (numpy.ndarray): Memory state\n",
        "        seed_ix (int): Seed letter for the first time step\n",
        "        n (int): Number of steps to sample\n",
        "\n",
        "    Returns:\n",
        "        list of int: Sequence of sampled integers\n",
        "    \"\"\"\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[seed_ix] = 1\n",
        "    ixes = []\n",
        "\n",
        "    for t in range(n):\n",
        "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "        y = np.dot(Why, h) + by\n",
        "        p = np.exp(y) / np.sum(np.exp(y))\n",
        "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[ix] = 1\n",
        "        ixes.append(ix)\n",
        "\n",
        "    return ixes"
      ],
      "metadata": {
        "id": "ATrWZtyJtdz1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # Memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # Loss at iteration 0\n",
        "\n",
        "acceptable_loss = 1.5  # Set an acceptable loss threshold\n",
        "max_iterations = 50000  # Set a maximum number of iterations to avoid infinite loop\n",
        "validation_interval = 10000  # Evaluate validation loss every 10000 iterations\n",
        "validation_data = open('/content/validation.txt', 'r').read()  # Load validation data\n",
        "\n",
        "best_loss = float('inf')  # Initialize best validation loss\n",
        "best_params = None  # Store best model parameters"
      ],
      "metadata": {
        "id": "uQ5UiRZ11Lx2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = (\n",
        "    hidden_size * vocab_size  # Wxh\n",
        "    + hidden_size * hidden_size  # Whh\n",
        "    + vocab_size * hidden_size  # Why\n",
        "    + 2 * hidden_size + vocab_size  # bh and by\n",
        ")\n",
        "\n",
        "print(\"Total number of parameters:\", total_params)\n",
        "\n",
        "# outputs the total number of parameters in the model\n",
        "# keep this similar to the size of i/p file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXSUEBC76LaK",
        "outputId": "cc03cab1-8028-4f59-df00-1945ac20062c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 81438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Loop:**\n",
        "\n",
        "The main training loop iterates through the data multiple times (controlled by `max_iterations`). In each iteration, it processes a chunk of `seq_length` characters, calculates loss, performs gradient updates using Adagrad, and updates the iteration and data pointers.\n",
        "* The inputs and targets for the current sequence are prepared.\n",
        "* The loss and gradients are calculated using the `lossFun` function.\n",
        "* Adagrad updates are applied to the model parameters.\n",
        "* The validation loss is evaluated every `validation_interval` iterations.\n",
        "\n",
        "### **Validation:**\n",
        "\n",
        "During the training loop, the code calculates the validation loss by processing the validation data in chunks of `seq_length` characters. It compares the validation loss to the best recorded loss and performs early stopping if the validation loss increases for an extended period."
      ],
      "metadata": {
        "id": "JZi5mAtv-bAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "while n < max_iterations:\n",
        "    # Prepare inputs (sweeping from left to right in steps seq_length long)\n",
        "    if p + seq_length + 1 >= len(data) or n == 0:\n",
        "        hprev = np.zeros((hidden_size, 1))  # Reset RNN memory\n",
        "        p = 0  # Go from the start of the data\n",
        "\n",
        "    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
        "    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
        "\n",
        "    # Sample from the model now and then\n",
        "    # if n % 100 == 0:\n",
        "    #     sample_ix = sample(hprev, inputs[0], 200)\n",
        "    #     txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    #     print('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "    # Forward seq_length characters through the net and fetch gradient\n",
        "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "    if n % 5000 == 0:\n",
        "        print('iter %d, loss: %f' % (n, smooth_loss))  # Print progress\n",
        "\n",
        "     # Check if the loss is acceptable\n",
        "    if smooth_loss < acceptable_loss:\n",
        "        print(\"Acceptable loss reached. Training complete.\")\n",
        "        break  # Exit the loop when acceptable loss is reached\n",
        "\n",
        "    # Perform parameter update with Adagrad\n",
        "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # Adagrad update\n",
        "\n",
        "    p += seq_length  # Move data pointer\n",
        "    n += 1  # Iteration counter\n",
        "\n",
        "    if n % validation_interval == 0:\n",
        "        val_loss = 0\n",
        "        num_batches = len(validation_data) // seq_length\n",
        "        val_hprev = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Calculate validation loss by iterating through validation data\n",
        "        for b in range(num_batches):\n",
        "            inputs = [char_to_ix[ch] for ch in validation_data[b*seq_length:(b+1)*seq_length]]\n",
        "            targets = [char_to_ix[ch] for ch in validation_data[b*seq_length+1:(b+1)*seq_length+1]]\n",
        "            loss, _, _, _, _, _, _ = lossFun(inputs, targets, val_hprev)\n",
        "            val_loss += loss\n",
        "\n",
        "        val_loss /= num_batches\n",
        "\n",
        "        print('iter %d, validation loss: %f' % (n, val_loss))  # Print validation progress\n",
        "\n",
        "        # Check for early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_params = {\n",
        "                'Wxh': np.copy(Wxh),\n",
        "                'Whh': np.copy(Whh),\n",
        "                'Why': np.copy(Why),\n",
        "                'bh': np.copy(bh),\n",
        "                'by': np.copy(by)\n",
        "            }\n",
        "\n",
        "        # Early stopping if validation loss starts increasing\n",
        "        if n > 2 * validation_interval and val_loss > 1.5 * best_loss:\n",
        "            print(\"Early stopping due to increasing validation loss.\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtYOf5MatgpO",
        "outputId": "c26c7675-cf2e-440b-8a72-de8949e53e86"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss: 85.029927\n",
            "iter 5000, loss: 65.834783\n",
            "iter 10000, validation loss: 64.450059\n",
            "iter 10000, loss: 57.007958\n",
            "iter 15000, loss: 54.612780\n",
            "iter 20000, validation loss: 65.641756\n",
            "iter 20000, loss: 53.310195\n",
            "iter 25000, loss: 52.044722\n",
            "iter 30000, validation loss: 65.731041\n",
            "iter 30000, loss: 51.091996\n",
            "iter 35000, loss: 50.312399\n",
            "iter 40000, validation loss: 65.672620\n",
            "iter 40000, loss: 49.750033\n",
            "iter 45000, loss: 49.077974\n",
            "iter 50000, validation loss: 72.005383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if we train for around 50,000 iterations even though our model might overfit, it will generate good text."
      ],
      "metadata": {
        "id": "KlOqt3Li_U7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's generally a good practice to ensure that the text length of the validation data is similar to that of the training data. This helps the model to generalize well to different text lengths\n",
        "and produce meaningful text during validation."
      ],
      "metadata": {
        "id": "4b77z3Mdtsh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text Generation:**\n",
        "\n",
        "After training, the best model parameters are used for text generation. A `starting character` is provided, and the `sample function` generates a sequence of characters by repeatedly predicting the next character based on the RNN's outputs."
      ],
      "metadata": {
        "id": "A91tMh1U_FVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the best model parameters for text generation\n",
        "Wxh = best_params['Wxh']\n",
        "Whh = best_params['Whh']\n",
        "Why = best_params['Why']\n",
        "bh = best_params['bh']\n",
        "by = best_params['by']\n",
        "\n",
        "# Specify the starting character for text generation\n",
        "starting_char = 'f'  # Replace with the character you want as the starting point\n",
        "starting_ix = char_to_ix[starting_char]\n",
        "\n",
        "# Generate text after training, starting with the specified character\n",
        "generated_text = sample(np.zeros((hidden_size, 1)), starting_ix, 1000)\n",
        "generated_text = ''.join(ix_to_char[ix] for ix in generated_text)\n",
        "print(\"Generated Text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwzRJ2HmvRG0",
        "outputId": "70f23586-bce2-407c-9ca4-7f9fdab18b8b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " or thes for id toatetisheukes praukger me ald ungish fo coauchaif magt nis oret thel\n",
            "fin the zoubut uy wiso ael pcaves wounk.\n",
            "pous dor\n",
            "thene lor loun hien ehaf lor is uet berpfor ante peag\n",
            "faardu plfer sof\n",
            "ayirawtore sorl faakail bi hin sume irefraesd ave siot for to lorsee psaa s-b lpmeoo say tre prave fatw bues\n",
            "fnig\n",
            "indrouthe yeas bus lom hial ou ape ang\n",
            "me as ser ixdhif wisol thans yofe pey\n",
            "foue puc bel ip chintsmexragu poy fawort nk werhe out aveld meaveaud cod my ter ikif he bes brer laat apel lone ar i peale pof wu sloranat oach thaklorit sniep\n",
            "bu hix.\n",
            "ft ale pupin to ali broues sof d\n",
            "lt\n",
            "ry mat oas louris alge if bung pu s bref\n",
            "thes bp ic foon wuly ing\n",
            "lly tin be pr dous co sicpe ore love irinorippre brt avys lif av mess ii hlmi ti sse reer sser praf you upuche pre all ald.\n",
            "prat prey pam dore loa fin tnl hay shi far tagp hiar pp or the bo thor foul whoet bu ul the iy silm gul\n",
            "arts hous z bu yold\n",
            "soming y the be sur oupr nlat ave\n",
            "arous buth barinseror mesirt liureevy soreare ove d\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **☝ GARBAGE**"
      ],
      "metadata": {
        "id": "bp9aeGjVCb-8"
      }
    }
  ]
}
